{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxSIyB92D3sk"
   },
   "source": [
    "## Sentiment analysis using tweets in Croatian\n",
    "- tweets are translated to english\n",
    "- words in the tweet are represented by pre-trained word2vec embedding\n",
    "- mean vector of words in a tweet are used as input features\n",
    "- models are trained and tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHeYn87J_57O"
   },
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yOgR_yc_XFW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for data handling\n",
    "import numpy as np # for linear algebra\n",
    "import time # for timing\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "from gensim.models import KeyedVectors # for pre-trained embedding \n",
    "\n",
    "from sklearn.model_selection import train_test_split # for reserving test data\n",
    "\n",
    "# metrics for model evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# sklearn classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf # for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paG0XXPGEXoi"
   },
   "source": [
    "#### Load pre-trained word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "tUwxjhph_m0m",
    "outputId": "d2f7e50f-96ab-4ab7-ba1d-6d52640f537d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-16 14:13:18--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.144.101\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.144.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve embedding file using wget\n",
    "# use this if embedding file is not available locally\n",
    "URL = \"https://s3.amazonaws.com/dl4j-distribution/\" # source url\n",
    "FILE = \"GoogleNews-vectors-negative300.bin.gz\" # source file name\n",
    "SOURCE = URL+FILE # source for embedding file\n",
    "DIR = \"/root/input/\" # directory\n",
    "! wget -P \"$DIR\" -c \"$SOURCE\" # retrieve embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "m6N3F1qeAJi4",
    "outputId": "3a10090b-35ab-4bbe-d8c1-04ceae248c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 3000000 words\n",
      "Each word is represented by a 300 dimensional vector\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained word2vec model from embedding file\n",
    "EMBEDDING_FILE = DIR + FILE \n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "# Define vocabulary and embedding_size\n",
    "vocabulary = set(word2vec.index2word) # set of words in vocabulary\n",
    "embedding_size = word2vec.vector_size # dimension of word vector\n",
    "print(\"Model contains %d words\" %len(vocabulary))\n",
    "print(\"Each word is represented by a %d dimensional vector\" %embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94u76zj_FGhs"
   },
   "source": [
    "#### Read data file with labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "HSOpVSuVELch",
    "outputId": "b4896d60-461f-4087-b6fc-c7bf1e434193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets = 3554.\n",
      "Number of labels = 3554.\n",
      "Number of missing tweets: 1241.\n",
      "Number of distinct tweets = 1869\n"
     ]
    }
   ],
   "source": [
    "DATAFILE = \"croatian_cl.csv\" # data file with labeled tweets\n",
    "df = pd.read_csv(DATAFILE) # read file\n",
    "\n",
    "# get tweets and targets\n",
    "tweets, targets = df.en.values, df.target.values\n",
    "\n",
    "print('Number of tweets = %d.' %len(tweets))\n",
    "print('Number of labels = %d.' %len(targets))\n",
    "\n",
    "# check for missing values and distinct tweets\n",
    "print(\"Number of missing tweets: %d.\" %(np.sum(pd.isnull(tweets))))\n",
    "print(\"Number of distinct tweets = %d\" %(df.en.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TF8ilPJkdWqm"
   },
   "source": [
    "#### Define function to obtain vector representation of a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXZROtfLWkUI"
   },
   "outputs": [],
   "source": [
    "def mean_vector(tweet):\n",
    "  \"\"\"Returns mean of vector representation words in tweet.\n",
    "  Returns a vector of zeros if none of the words appear in vocabulary \n",
    "  See: https://github.com/USC-CSSL/DDR \"\"\"\n",
    "  zero = np.zeros((embedding_size,), dtype=\"float32\") # for null tweet\n",
    "  if pd.isnull(tweet): return zero # tweet missing\n",
    "  words = [w for w in tweet.split() if w in vocabulary] # valid words\n",
    "  if not words: return zero # no word in vocabulary\n",
    "  return np.mean([word2vec[w] for w in words], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QW7R1PlpdiQX"
   },
   "source": [
    "#### Create a new dataframe with vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "lFz8RnD8ZAMi",
    "outputId": "352fdb65-c2c4-4b3b-f632-469598a5a0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation has 3554 rows and 301 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>v_0</th>\n",
       "      <th>v_1</th>\n",
       "      <th>v_2</th>\n",
       "      <th>v_3</th>\n",
       "      <th>v_4</th>\n",
       "      <th>v_5</th>\n",
       "      <th>v_6</th>\n",
       "      <th>v_7</th>\n",
       "      <th>v_8</th>\n",
       "      <th>v_9</th>\n",
       "      <th>v_10</th>\n",
       "      <th>v_11</th>\n",
       "      <th>v_12</th>\n",
       "      <th>v_13</th>\n",
       "      <th>v_14</th>\n",
       "      <th>v_15</th>\n",
       "      <th>v_16</th>\n",
       "      <th>v_17</th>\n",
       "      <th>v_18</th>\n",
       "      <th>v_19</th>\n",
       "      <th>v_20</th>\n",
       "      <th>v_21</th>\n",
       "      <th>v_22</th>\n",
       "      <th>v_23</th>\n",
       "      <th>v_24</th>\n",
       "      <th>v_25</th>\n",
       "      <th>v_26</th>\n",
       "      <th>v_27</th>\n",
       "      <th>v_28</th>\n",
       "      <th>v_29</th>\n",
       "      <th>v_30</th>\n",
       "      <th>v_31</th>\n",
       "      <th>v_32</th>\n",
       "      <th>v_33</th>\n",
       "      <th>v_34</th>\n",
       "      <th>v_35</th>\n",
       "      <th>v_36</th>\n",
       "      <th>v_37</th>\n",
       "      <th>v_38</th>\n",
       "      <th>...</th>\n",
       "      <th>v_260</th>\n",
       "      <th>v_261</th>\n",
       "      <th>v_262</th>\n",
       "      <th>v_263</th>\n",
       "      <th>v_264</th>\n",
       "      <th>v_265</th>\n",
       "      <th>v_266</th>\n",
       "      <th>v_267</th>\n",
       "      <th>v_268</th>\n",
       "      <th>v_269</th>\n",
       "      <th>v_270</th>\n",
       "      <th>v_271</th>\n",
       "      <th>v_272</th>\n",
       "      <th>v_273</th>\n",
       "      <th>v_274</th>\n",
       "      <th>v_275</th>\n",
       "      <th>v_276</th>\n",
       "      <th>v_277</th>\n",
       "      <th>v_278</th>\n",
       "      <th>v_279</th>\n",
       "      <th>v_280</th>\n",
       "      <th>v_281</th>\n",
       "      <th>v_282</th>\n",
       "      <th>v_283</th>\n",
       "      <th>v_284</th>\n",
       "      <th>v_285</th>\n",
       "      <th>v_286</th>\n",
       "      <th>v_287</th>\n",
       "      <th>v_288</th>\n",
       "      <th>v_289</th>\n",
       "      <th>v_290</th>\n",
       "      <th>v_291</th>\n",
       "      <th>v_292</th>\n",
       "      <th>v_293</th>\n",
       "      <th>v_294</th>\n",
       "      <th>v_295</th>\n",
       "      <th>v_296</th>\n",
       "      <th>v_297</th>\n",
       "      <th>v_298</th>\n",
       "      <th>v_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.065150</td>\n",
       "      <td>0.009019</td>\n",
       "      <td>0.067179</td>\n",
       "      <td>-0.040919</td>\n",
       "      <td>-0.012410</td>\n",
       "      <td>-0.018280</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>-0.134450</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.074039</td>\n",
       "      <td>-0.049042</td>\n",
       "      <td>-0.054626</td>\n",
       "      <td>0.049611</td>\n",
       "      <td>0.055868</td>\n",
       "      <td>-0.001475</td>\n",
       "      <td>0.069340</td>\n",
       "      <td>-0.024841</td>\n",
       "      <td>0.050489</td>\n",
       "      <td>-0.006887</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>0.118340</td>\n",
       "      <td>-0.060832</td>\n",
       "      <td>-0.050171</td>\n",
       "      <td>0.029507</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>-0.058272</td>\n",
       "      <td>0.081014</td>\n",
       "      <td>0.095500</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>0.054428</td>\n",
       "      <td>-0.094330</td>\n",
       "      <td>-0.033971</td>\n",
       "      <td>-0.046718</td>\n",
       "      <td>0.024851</td>\n",
       "      <td>0.039581</td>\n",
       "      <td>-0.059804</td>\n",
       "      <td>0.017680</td>\n",
       "      <td>-0.021286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097619</td>\n",
       "      <td>-0.012993</td>\n",
       "      <td>0.059367</td>\n",
       "      <td>0.107656</td>\n",
       "      <td>0.056512</td>\n",
       "      <td>0.132039</td>\n",
       "      <td>-0.091136</td>\n",
       "      <td>-0.055033</td>\n",
       "      <td>-0.079895</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>0.068047</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.163801</td>\n",
       "      <td>-0.003601</td>\n",
       "      <td>0.062683</td>\n",
       "      <td>-0.096575</td>\n",
       "      <td>-0.010091</td>\n",
       "      <td>-0.012889</td>\n",
       "      <td>0.030141</td>\n",
       "      <td>0.020559</td>\n",
       "      <td>-0.069656</td>\n",
       "      <td>0.062251</td>\n",
       "      <td>0.045340</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>-0.082072</td>\n",
       "      <td>-0.016469</td>\n",
       "      <td>-0.011396</td>\n",
       "      <td>0.047234</td>\n",
       "      <td>-0.171183</td>\n",
       "      <td>0.040507</td>\n",
       "      <td>-0.103552</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>0.008240</td>\n",
       "      <td>0.063232</td>\n",
       "      <td>-0.029500</td>\n",
       "      <td>-0.023102</td>\n",
       "      <td>0.063029</td>\n",
       "      <td>0.006251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.047030</td>\n",
       "      <td>0.038298</td>\n",
       "      <td>0.050593</td>\n",
       "      <td>-0.014921</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.012733</td>\n",
       "      <td>-0.126671</td>\n",
       "      <td>0.135160</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>-0.032330</td>\n",
       "      <td>-0.042847</td>\n",
       "      <td>0.013057</td>\n",
       "      <td>0.079890</td>\n",
       "      <td>0.013024</td>\n",
       "      <td>0.045451</td>\n",
       "      <td>0.021217</td>\n",
       "      <td>0.058042</td>\n",
       "      <td>-0.033532</td>\n",
       "      <td>0.021067</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>0.130007</td>\n",
       "      <td>-0.046575</td>\n",
       "      <td>-0.063240</td>\n",
       "      <td>0.043275</td>\n",
       "      <td>-0.040901</td>\n",
       "      <td>-0.065597</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>0.093467</td>\n",
       "      <td>-0.014897</td>\n",
       "      <td>0.043349</td>\n",
       "      <td>-0.076003</td>\n",
       "      <td>-0.073219</td>\n",
       "      <td>-0.038868</td>\n",
       "      <td>0.008141</td>\n",
       "      <td>0.030405</td>\n",
       "      <td>-0.055598</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>-0.008437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066193</td>\n",
       "      <td>-0.016632</td>\n",
       "      <td>0.057467</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.018811</td>\n",
       "      <td>0.139160</td>\n",
       "      <td>-0.087874</td>\n",
       "      <td>-0.046190</td>\n",
       "      <td>-0.113563</td>\n",
       "      <td>-0.025668</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>0.048382</td>\n",
       "      <td>0.051730</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.138824</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>-0.107699</td>\n",
       "      <td>-0.008357</td>\n",
       "      <td>-0.025269</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>-0.076594</td>\n",
       "      <td>0.036316</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.029917</td>\n",
       "      <td>-0.076397</td>\n",
       "      <td>-0.025306</td>\n",
       "      <td>-0.013787</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>-0.201397</td>\n",
       "      <td>0.022677</td>\n",
       "      <td>-0.098239</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>0.060636</td>\n",
       "      <td>-0.025118</td>\n",
       "      <td>-0.056688</td>\n",
       "      <td>0.061768</td>\n",
       "      <td>-0.018038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>-0.007246</td>\n",
       "      <td>0.036150</td>\n",
       "      <td>0.069449</td>\n",
       "      <td>-0.058742</td>\n",
       "      <td>0.028303</td>\n",
       "      <td>0.021938</td>\n",
       "      <td>-0.074825</td>\n",
       "      <td>0.170131</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>-0.057931</td>\n",
       "      <td>-0.105502</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.026956</td>\n",
       "      <td>-0.083705</td>\n",
       "      <td>0.082903</td>\n",
       "      <td>0.018865</td>\n",
       "      <td>0.061611</td>\n",
       "      <td>-0.054331</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>-0.043265</td>\n",
       "      <td>-0.007756</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.003996</td>\n",
       "      <td>-0.029933</td>\n",
       "      <td>0.025635</td>\n",
       "      <td>-0.111973</td>\n",
       "      <td>0.044713</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>0.029079</td>\n",
       "      <td>-0.014352</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.034337</td>\n",
       "      <td>-0.009421</td>\n",
       "      <td>-0.001295</td>\n",
       "      <td>-0.046138</td>\n",
       "      <td>0.016440</td>\n",
       "      <td>-0.015268</td>\n",
       "      <td>0.038993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015682</td>\n",
       "      <td>-0.029837</td>\n",
       "      <td>-0.032270</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>-0.000750</td>\n",
       "      <td>0.162074</td>\n",
       "      <td>0.047036</td>\n",
       "      <td>-0.043206</td>\n",
       "      <td>-0.068412</td>\n",
       "      <td>-0.036375</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.075383</td>\n",
       "      <td>0.084787</td>\n",
       "      <td>-0.001395</td>\n",
       "      <td>0.069188</td>\n",
       "      <td>-0.077698</td>\n",
       "      <td>-0.042717</td>\n",
       "      <td>-0.117074</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>-0.031363</td>\n",
       "      <td>-0.003601</td>\n",
       "      <td>0.014319</td>\n",
       "      <td>-0.020796</td>\n",
       "      <td>0.052702</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.038986</td>\n",
       "      <td>0.018559</td>\n",
       "      <td>0.040728</td>\n",
       "      <td>0.043115</td>\n",
       "      <td>-0.098458</td>\n",
       "      <td>0.033447</td>\n",
       "      <td>-0.101135</td>\n",
       "      <td>-0.017648</td>\n",
       "      <td>-0.030831</td>\n",
       "      <td>0.060713</td>\n",
       "      <td>-0.018651</td>\n",
       "      <td>-0.043719</td>\n",
       "      <td>0.074031</td>\n",
       "      <td>-0.026228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.219910</td>\n",
       "      <td>0.014183</td>\n",
       "      <td>-0.025162</td>\n",
       "      <td>-0.056625</td>\n",
       "      <td>-0.002243</td>\n",
       "      <td>-0.026909</td>\n",
       "      <td>-0.141098</td>\n",
       "      <td>0.039261</td>\n",
       "      <td>0.091843</td>\n",
       "      <td>0.033615</td>\n",
       "      <td>-0.044662</td>\n",
       "      <td>-0.044727</td>\n",
       "      <td>0.016409</td>\n",
       "      <td>-0.107941</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>-0.055153</td>\n",
       "      <td>0.063171</td>\n",
       "      <td>-0.040625</td>\n",
       "      <td>-0.116760</td>\n",
       "      <td>-0.049404</td>\n",
       "      <td>0.114670</td>\n",
       "      <td>-0.082092</td>\n",
       "      <td>-0.033765</td>\n",
       "      <td>0.059723</td>\n",
       "      <td>-0.100128</td>\n",
       "      <td>-0.078390</td>\n",
       "      <td>0.114548</td>\n",
       "      <td>0.148252</td>\n",
       "      <td>-0.082382</td>\n",
       "      <td>-0.007370</td>\n",
       "      <td>0.067368</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.038830</td>\n",
       "      <td>-0.033752</td>\n",
       "      <td>-0.085182</td>\n",
       "      <td>-0.019455</td>\n",
       "      <td>-0.037857</td>\n",
       "      <td>0.086548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029911</td>\n",
       "      <td>-0.034008</td>\n",
       "      <td>-0.039093</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.149231</td>\n",
       "      <td>0.147850</td>\n",
       "      <td>-0.031239</td>\n",
       "      <td>0.034821</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>0.023254</td>\n",
       "      <td>0.050354</td>\n",
       "      <td>0.055450</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.091095</td>\n",
       "      <td>-0.075897</td>\n",
       "      <td>-0.047775</td>\n",
       "      <td>-0.149414</td>\n",
       "      <td>0.024166</td>\n",
       "      <td>-0.057501</td>\n",
       "      <td>0.070264</td>\n",
       "      <td>0.043154</td>\n",
       "      <td>0.116257</td>\n",
       "      <td>-0.005800</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.041729</td>\n",
       "      <td>-0.125759</td>\n",
       "      <td>0.128830</td>\n",
       "      <td>-0.031479</td>\n",
       "      <td>0.106674</td>\n",
       "      <td>-0.093201</td>\n",
       "      <td>-0.077744</td>\n",
       "      <td>-0.122597</td>\n",
       "      <td>-0.011139</td>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.000778</td>\n",
       "      <td>-0.084488</td>\n",
       "      <td>-0.060814</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>-0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.173340</td>\n",
       "      <td>-0.013428</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>0.087097</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>0.200806</td>\n",
       "      <td>-0.009323</td>\n",
       "      <td>0.252197</td>\n",
       "      <td>0.140381</td>\n",
       "      <td>-0.087158</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>-0.051636</td>\n",
       "      <td>0.019287</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>0.214874</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.073486</td>\n",
       "      <td>-0.094482</td>\n",
       "      <td>0.081177</td>\n",
       "      <td>0.180176</td>\n",
       "      <td>0.153564</td>\n",
       "      <td>0.158356</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>0.081657</td>\n",
       "      <td>0.079773</td>\n",
       "      <td>0.169434</td>\n",
       "      <td>-0.198730</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>-0.034668</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.085541</td>\n",
       "      <td>-0.024170</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>-0.054688</td>\n",
       "      <td>0.015778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034485</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>0.044128</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>0.024902</td>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.094971</td>\n",
       "      <td>0.130615</td>\n",
       "      <td>-0.042480</td>\n",
       "      <td>0.038574</td>\n",
       "      <td>0.196289</td>\n",
       "      <td>0.172363</td>\n",
       "      <td>0.107666</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.173523</td>\n",
       "      <td>-0.036743</td>\n",
       "      <td>-0.061768</td>\n",
       "      <td>-0.098633</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>-0.040710</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>-0.022217</td>\n",
       "      <td>0.056030</td>\n",
       "      <td>-0.049805</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>-0.043976</td>\n",
       "      <td>0.033585</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>-0.265625</td>\n",
       "      <td>-0.055786</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>-0.021978</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>-0.019775</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.022995</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>0.040401</td>\n",
       "      <td>-0.038757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target       v_0       v_1       v_2  ...     v_296     v_297     v_298     v_299\n",
       "0       0  0.065150  0.009019  0.067179  ... -0.029500 -0.023102  0.063029  0.006251\n",
       "1       0  0.047030  0.038298  0.050593  ... -0.025118 -0.056688  0.061768 -0.018038\n",
       "2       0  0.011346 -0.007246  0.036150  ... -0.018651 -0.043719  0.074031 -0.026228\n",
       "3       0  0.018066  0.219910  0.014183  ... -0.084488 -0.060814  0.055687 -0.043800\n",
       "4       0  0.173340 -0.013428  0.019592  ... -0.022995 -0.115967  0.040401 -0.038757\n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['v_'+str(i) for i in range(embedding_size)] # column names\n",
    "dfV = pd.DataFrame(list(map(mean_vector, tweets)), columns=cols) # new df\n",
    "dfV.insert(0, 'target', df.target) # insert label for tweets\n",
    "print('Representation has %d rows and %d columns' %dfV.shape)\n",
    "dfV.to_csv(\"vectors_croatian.csv\", index=False) # save as csv file\n",
    "dfV.head() # display first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9_9SuW-UnsJ6"
   },
   "source": [
    "#### Reserve testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ugSvYLenkMBG",
    "outputId": "4f1aae47-ffae-458b-e99a-0edc6c5bb87e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains 2843 examples\n",
      "Test set contains 711 examples\n",
      "Number of input features = 300\n"
     ]
    }
   ],
   "source": [
    "features = list(dfV)[1:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfV[features], dfV['target'], test_size=0.2, random_state=2019)\n",
    "print(\"Training set contains %d examples\" %len(X_train))\n",
    "print(\"Test set contains %d examples\" %len(X_test))\n",
    "print(\"Number of input features = %d\" %len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "SAFqVS56o2Fp",
    "outputId": "94574b68-1de4-4236-d9d2-38dda3cab52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution in training and test data:\n",
      "              0         1\n",
      "train  0.404151  0.595849\n",
      "test   0.388186  0.611814\n",
      "\n",
      "Class counts in training and test data:\n",
      "          0     1\n",
      "train  1149  1694\n",
      "test    276   435\n"
     ]
    }
   ],
   "source": [
    "# Check distribution of target in training and validation data\n",
    "def classDistribution(y_train, y_test):\n",
    "    \"\"\"Returns distribution of classes in training and test data\"\"\"\n",
    "    res, resCount = pd.DataFrame(), pd.DataFrame()\n",
    "    res['train'] = y_train.value_counts(normalize=True, sort=False)\n",
    "    res['test'] = y_test.value_counts(normalize=True, sort=False)\n",
    "    resCount['train'] = y_train.value_counts(normalize=False, sort=False)\n",
    "    resCount['test'] = y_test.value_counts(normalize=False, sort=False)\n",
    "    return res.transpose(), resCount.transpose()\n",
    "\n",
    "dist, count = classDistribution(y_train, y_test)\n",
    "print('\\nClass distribution in training and test data:')\n",
    "print(dist)\n",
    "print('\\nClass counts in training and test data:')\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M954Em8jrQu3"
   },
   "source": [
    "#### Train and test models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CcNlZfu4meZ"
   },
   "source": [
    "##### Specify function for evaluating trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLWugQMMrU3D"
   },
   "outputs": [],
   "source": [
    "def evaluateModel(y_test, predicted):\n",
    "    \"\"\"evaluates trained model\"\"\"\n",
    "    acc = accuracy_score(y_test, predicted) # accuracy\n",
    "    print(\"\\nAccuracy with validation data: %4.2f%%\" %(100*acc))\n",
    "    print(\"\\nClassification report:\\n\")\n",
    "    print(classification_report(y_test, predicted)) \n",
    "    cm = confusion_matrix(y_test, predicted) # confusion_matrix\n",
    "    print(\"\\nConfusion matrix:\\n\")\n",
    "    print(pd.DataFrame(cm))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wb3x9JMEtBKZ"
   },
   "source": [
    "##### Specify models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8QgKGxvyzEQ"
   },
   "outputs": [],
   "source": [
    "models = {} # dictionary of Scikit-Learn classifiers with non-default parameters \n",
    "models['DT'] = DecisionTreeClassifier()\n",
    "models['RF'] = RandomForestClassifier(n_estimators=100)\n",
    "models['SVM'] = SVC(kernel='poly', gamma='scale')\n",
    "models['KNN'] = KNeighborsClassifier(n_neighbors=3)\n",
    "models['LRM'] = LogisticRegression(multi_class='auto', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVRhDTSb7Okf"
   },
   "source": [
    "##### Train and test Scikit-Learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2652
    },
    "colab_type": "code",
    "id": "gnl-lpJmzJy2",
    "outputId": "156095d7-f15c-4173-9eb5-8ee73d4562d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training classifier DT:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Time to train and test classifier: 0.55 seconds\n",
      "\n",
      "Accuracy with validation data: 81.01%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.75       276\n",
      "           1       0.83      0.86      0.85       435\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       711\n",
      "   macro avg       0.80      0.79      0.80       711\n",
      "weighted avg       0.81      0.81      0.81       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  200   76\n",
      "1   59  376\n",
      "============================================================\n",
      "\n",
      "Training classifier RF:\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Time to train and test classifier: 1.99 seconds\n",
      "\n",
      "Accuracy with validation data: 87.90%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       276\n",
      "           1       0.91      0.89      0.90       435\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       711\n",
      "   macro avg       0.87      0.88      0.87       711\n",
      "weighted avg       0.88      0.88      0.88       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  240   36\n",
      "1   50  385\n",
      "============================================================\n",
      "\n",
      "Training classifier SVM:\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Time to train and test classifier: 1.77 seconds\n",
      "\n",
      "Accuracy with validation data: 88.05%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       276\n",
      "           1       0.92      0.88      0.90       435\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       711\n",
      "   macro avg       0.87      0.88      0.88       711\n",
      "weighted avg       0.88      0.88      0.88       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  245   31\n",
      "1   54  381\n",
      "============================================================\n",
      "\n",
      "Training classifier KNN:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Time to train and test classifier: 0.94 seconds\n",
      "\n",
      "Accuracy with validation data: 82.70%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.79       276\n",
      "           1       0.90      0.81      0.85       435\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       711\n",
      "   macro avg       0.82      0.83      0.82       711\n",
      "weighted avg       0.84      0.83      0.83       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  236   40\n",
      "1   83  352\n",
      "============================================================\n",
      "\n",
      "Training classifier LRM:\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "Time to train and test classifier: 0.06 seconds\n",
      "\n",
      "Accuracy with validation data: 85.37%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       276\n",
      "           1       0.90      0.85      0.88       435\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       711\n",
      "   macro avg       0.84      0.85      0.85       711\n",
      "weighted avg       0.86      0.85      0.85       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  236   40\n",
      "1   64  371\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.546197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.879044</td>\n",
       "      <td>1.993088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.880450</td>\n",
       "      <td>1.769954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.827004</td>\n",
       "      <td>0.936793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LRM</td>\n",
       "      <td>0.853727</td>\n",
       "      <td>0.062253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  accuracy      time\n",
       "0    DT  0.810127  0.546197\n",
       "1    RF  0.879044  1.993088\n",
       "2   SVM  0.880450  1.769954\n",
       "3   KNN  0.827004  0.936793\n",
       "4   LRM  0.853727  0.062253"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for m in [m for m in models]:\n",
    "    model = models[m] # model to use\n",
    "    print(\"\\nTraining classifier %s:\\n%s\" %(m, model))\n",
    "    st = time.time() # start time for training and testing\n",
    "    model.fit(X_train, y_train) # train model\n",
    "    predicted = model.predict(X_test) # predict test labels with trained model\n",
    "    t = time.time() - st # time to train and test model\n",
    "    print(\"Time to train and test classifier: %4.2f seconds\" %(t))\n",
    "    acc = evaluateModel(y_test, predicted) # evaluate prediction accuracy\n",
    "    result.append([m, acc, t]) # record results\n",
    "    print(60*'=') # end training and testing for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIg1WZAm7nlq"
   },
   "source": [
    "##### Use single-layered tensorflow neural network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qs-jnuKa7lb4"
   },
   "outputs": [],
   "source": [
    "# Define single-layered tensorflow neural network (ANN)\n",
    "def ann(**kwargs):\n",
    "    \"\"\"Returns trained single layered network\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(kwargs['nFeatures'],)),\n",
    "        tf.keras.layers.Dense(kwargs['nNeurons'], activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(kwargs['dropOutRate']),\n",
    "        tf.keras.layers.Dense(kwargs['nClasses'], activation=tf.nn.softmax)])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Shr2DDhT79bK"
   },
   "outputs": [],
   "source": [
    "# Specify model parameters for ANN\n",
    "nNeurons = 128 # number of neurons in hidden layer\n",
    "dropOutRate = 0.2 # drop out rate\n",
    "nFeatures= embedding_size # number of input features\n",
    "nClasses = 2 # number of output classes\n",
    "nEpochs = 10 # number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1176
    },
    "colab_type": "code",
    "id": "_5ZCf1TG8OR3",
    "outputId": "1bcc617a-54e1-4375-a3e1-498da186f4a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 38,786\n",
      "Trainable params: 38,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Training classifier: ANN\n",
      "Train on 2274 samples, validate on 569 samples\n",
      "Epoch 1/10\n",
      "2274/2274 [==============================] - 1s 296us/sample - loss: 0.6131 - acc: 0.7533 - val_loss: 0.5226 - val_acc: 0.7996\n",
      "Epoch 2/10\n",
      "2274/2274 [==============================] - 0s 98us/sample - loss: 0.4344 - acc: 0.8382 - val_loss: 0.4083 - val_acc: 0.8366\n",
      "Epoch 3/10\n",
      "2274/2274 [==============================] - 0s 98us/sample - loss: 0.3470 - acc: 0.8646 - val_loss: 0.3876 - val_acc: 0.8471\n",
      "Epoch 4/10\n",
      "2274/2274 [==============================] - 0s 103us/sample - loss: 0.3174 - acc: 0.8764 - val_loss: 0.3898 - val_acc: 0.8418\n",
      "Epoch 5/10\n",
      "2274/2274 [==============================] - 0s 107us/sample - loss: 0.3013 - acc: 0.8826 - val_loss: 0.3906 - val_acc: 0.8489\n",
      "Epoch 6/10\n",
      "2274/2274 [==============================] - 0s 105us/sample - loss: 0.2927 - acc: 0.8905 - val_loss: 0.4061 - val_acc: 0.8330\n",
      "Epoch 7/10\n",
      "2274/2274 [==============================] - 0s 100us/sample - loss: 0.2850 - acc: 0.8984 - val_loss: 0.3973 - val_acc: 0.8471\n",
      "Epoch 8/10\n",
      "2274/2274 [==============================] - 0s 103us/sample - loss: 0.2737 - acc: 0.8953 - val_loss: 0.4082 - val_acc: 0.8366\n",
      "Epoch 9/10\n",
      "2274/2274 [==============================] - 0s 108us/sample - loss: 0.2679 - acc: 0.9059 - val_loss: 0.4065 - val_acc: 0.8453\n",
      "Epoch 10/10\n",
      "2274/2274 [==============================] - 0s 110us/sample - loss: 0.2594 - acc: 0.9072 - val_loss: 0.4211 - val_acc: 0.8366\n",
      "Time to train and test classifier: 3.77 seconds\n",
      "\n",
      "Accuracy with validation data: 85.79%\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       276\n",
      "           1       0.89      0.87      0.88       435\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       711\n",
      "   macro avg       0.85      0.85      0.85       711\n",
      "weighted avg       0.86      0.86      0.86       711\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "     0    1\n",
      "0  230   46\n",
      "1   55  380\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train and test ANN model\n",
    "m = 'ANN' # model name\n",
    "model = ann(nNeurons=nNeurons, dropOutRate=dropOutRate,\n",
    "            nFeatures=nFeatures, nClasses=nClasses) # specify model\n",
    "print(model.summary()) # display model summary\n",
    "print(\"\\nTraining classifier: %s\" %m)\n",
    "st = time.time() # start time for training and testing\n",
    "hist = model.fit(X_train, y_train, epochs=nEpochs, validation_split=0.2) # train\n",
    "predicted = model.predict(X_test) # predict test examples\n",
    "predicted = np.argmax(predicted, axis=1) # most likely label\n",
    "t = time.time() - st # time to train and test model\n",
    "print(\"Time to train and test classifier: %4.2f seconds\" %(t))\n",
    "acc = evaluateModel(y_test, predicted) # evaluate prediction accuracy\n",
    "result.append([m, acc, t]) # record results\n",
    "print(60*'=') # end training and testing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBkPtpjJ90gm"
   },
   "outputs": [],
   "source": [
    "# plot training and test accuracy\n",
    "def plotHistorty(model, history):\n",
    "    # plot history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title(model+' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "3pFfYAoN-C23",
    "outputId": "9925921e-a705-46b0-f7e9-f48dcc361b05"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntMvCwhhjTLssLIwxxrTr/wNFlgMvx4bEtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotHistorty(m, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyvtBoPQAt57"
   },
   "source": [
    "##### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "DFXzyr7hAylF",
    "outputId": "81d4f8f7-4723-4032-e984-915fd3eb6b40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.546197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.879044</td>\n",
       "      <td>1.993088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.880450</td>\n",
       "      <td>1.769954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.827004</td>\n",
       "      <td>0.936793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LRM</td>\n",
       "      <td>0.853727</td>\n",
       "      <td>0.062253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.857947</td>\n",
       "      <td>3.773772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  accuracy      time\n",
       "0    DT  0.810127  0.546197\n",
       "1    RF  0.879044  1.993088\n",
       "2   SVM  0.880450  1.769954\n",
       "3   KNN  0.827004  0.936793\n",
       "4   LRM  0.853727  0.062253\n",
       "5   ANN  0.857947  3.773772"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result, columns = ['model', 'accuracy', 'time']) # show results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Croatian_SA.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
